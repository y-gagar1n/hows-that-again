{"version":3,"sources":["webpack:///path---blog-video-microservices-what-we-learned-f5a90a189b44da7a91be.js","webpack:///./.cache/json/blog-video-microservices-what-we-learned.json"],"names":["webpackJsonp","488","module","exports","data","markdownRemark","html","frontmatter","path","title","pathContext"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,KAAA,uutBAA+2sBC,aAAkiBC,KAAA,6CAAAC,MAAA,kDAA6GC","file":"path---blog-video-microservices-what-we-learned-f5a90a189b44da7a91be.js","sourcesContent":["webpackJsonp([229226591425695],{\n\n/***/ 488:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<p>Кусок архитектуры М-Тех, который занимается нарезкой видео из лайв-потока:</p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/6f71d/microservices-example.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block;  max-width: 650px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 76.51296829971182%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACzklEQVQ4y11U207bQBDN//exP0AfqlZqKVQU8VJouYQASSMgaUwuduxgO/gSx/au73ZOxxtCoGOt1lrPnD0zc8YNkFVVhSiKkKYpQubDXc7BOUMcx2IxxpDnOd7YavtalgVY4om9sT4oEQQ+8rTEyOjgdPQRQexsY1crAZxnJbIkR5alyMsccRIjiVJoloTjhx3YgbYGrBnatk2sIni+A92R6Z0LxkmSIo5iyoDT+RBuaMCzlpgONAJL1pelnMBmyIpkCzg3TXAWQ9I6uBjsw3RUnPW/4e+sRQ7AMljg12AXkn2Daf8RRx9OwJYc/1tjk5Ku6wKwvmn01EaSM/SUS+juCCsCLMoKgd5FNO9TvQIM5DayPBEgPPGhOn0kGUejBquqNWDEtzeuSqD5/QaT2ylK8vEcC/bJO/h3B5AdCT/7OwgiR7CfLxWcjz/DZfqaYW2GbiAMQ1TlSjDKkgxSewRdniOqu02XzYxbWAtN+CdZJJpZEvMoDaHazwynFkN34mKqyDCNOXRbRl+9hOtZouO6M0YaZwQa4lT6it7j2Ru51Jk8+YpQhlMzvBq6+HKhwl96CH2OybyL5mgXi9BEc7hH9bzBpnGM+5SFT6VZ61MwLGhVJXgcCJ/Ga63VMnkt3PtmHwalXBATc2aiddTB+UGL9jZ+751DHWviW57lYoku10C11ZPwGrAgEbd+dDDqKgI99ENokoHJvQq5p0LpafAXwXZqnifnDSALavFO8GBckRQCjMw2TG8sOlnQZIxJTk+BgiCxIZktpEUkYn1uo6edgcXeFrDOP0sLDPUOruTv4JmHP4/HkN17VAWVI49xPTyEYt2RTGSc9D4hjBci1vI1NCdUd2Zuhf2Sal6ChZzkU8K+OwSfXoNkKjqqLfrUydmLb63f17EvKW8O670sqMjUuZgt4Tbfg8lrmeQ0p82HfUhUjs0PpV6bOLHo+Qen728FTQDSQgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Microservices example\\\"\\n        title=\\\"\\\"\\n        src=\\\"/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/10273/microservices-example.png\\\"\\n        srcset=\\\"/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/9b14a/microservices-example.png 163w,\\n/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/94962/microservices-example.png 325w,\\n/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/10273/microservices-example.png 650w,\\n/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/6f71d/microservices-example.png 694w\\\"\\n        sizes=\\\"(max-width: 650px) 100vw, 650px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p>Зеленым обозначены микросервисы. Здесь их 17, и это 1/20 часть всей системы. То есть всего около 340 микросервисов. С учетом того, что некоторые сервисы разворачиваются в нескольких экземплярах, получается 1120 нод.</p>\\n<h2>Насколько мелким должен быть микросервис?</h2>\\n<p>Настолько, чтобы он был независимым!</p>\\n<h3>Как проверить независимость:</h3>\\n<ol>\\n<li>Бизнес-задача сервиса должна описываться одним простым предложением. Если не получается - значит можно еще поделить. Например, \\\"показать картинки на таймлайне видеоредактора\\\". Если на эта картинку, например, нужно вотермарк наложить, то это надо делать в этом же сервисе. То есть делить нужно <strong>не технически</strong>, а по бизнес-задачам, иначе грануляция будет слишком высокая. Общие вещи - копипастятся и кладутся в отдельную папку vendor.</li>\\n<li>У сервиса должно быть больше одного потребителя. Если один - значит слишком мелко поделили.</li>\\n<li>Деплой сервиса не должен приводить к деплою других сервисов. Если приводит - значит есть зависимость нашего сервиса от другого, значит слишком мелко поделили.</li>\\n</ol>\\n<h3>Требования к микросервису:</h3>\\n<ul>\\n<li>скрывает внутренние детали реализации</li>\\n<li>деплоится независимо</li>\\n<li>падая, не роняет все остальное</li>\\n<li>легко мониторится</li>\\n<li>реализует бизнес-модель</li>\\n<li>полностью децентрализован</li>\\n</ul>\\n<h2>Конфигурация</h2>\\n<h3>Что пробовали?</h3>\\n<ul>\\n<li>файлы конфигурации при деплое разливать по всем целевым машинам. Минус: сложно менять конфигурацию для одной ноды, количество загрузок конфигов растет линейно с ростом машин.</li>\\n<li>файлы конфигурации класть в контейнер. Минус: при изменении конфига нужно пересобрать и передеплоить контейнет.</li>\\n<li>\\n<p>выставлять переменные окружения (рекомендуется в <a href=\\\"https://12factor.net/\\\">https://12factor.net/</a>)</p>\\n<ul>\\n<li>при сборке контейнера (<code>ENV DB=...</code>). Минус: на этапе деплоя нет информации, где это будет лежать</li>\\n<li>при запуске контейнера. Минус: на этапе запуска нет возможности абстрагироваться от системы запуска.</li>\\n</ul>\\n</li>\\n</ul>\\n<h3>Решение</h3>\\n<ul>\\n<li>общее K/V хранилище (у них consul)</li>\\n<li>сервис должен быть заточен под смену конфигурации</li>\\n<li>\\n<p>микс вариаций с приоритетами:</p>\\n<ul>\\n<li>&#x3C;СЕРВИС>/ (/conf/ms/recorder)</li>\\n<li>&#x3C;СЕРВИС>/&#x3C;ВЕРСИЯ>/ (/conf/ms/recorder/0.12)</li>\\n<li>&#x3C;СЕРВИС>/&#x3C;СРЕДА>/</li>\\n<li>&#x3C;СЕРВИС>/&#x3C;ВЕРСИЯ>/&#x3C;СРЕДА>/</li>\\n<li>&#x3C;СЕРВИС>/&#x3C;ВЕРСИЯ>/&#x3C;НОДА>/</li>\\n</ul>\\n</li>\\n</ul>\\n<p>Конфиги хранятся в тех же репозиториях, что и проекты.</p>\\n<p>Утилита <strong>git2consul</strong> при деплое загружает конфиги в consul.</p>\\n<p>Стартовый путь:\\n<code>/conf/ms/&#x3C;ИМЯ СЕРВИСА>/&#x3C;BUILD NO>/</code></p>\\n<p>Все секретные данные (сертификаты, пароли к внешним сервисам, соль) хранятся в отдельных проектах, загружаются в <strong>Vault</strong> (<a href=\\\"https://www.vaultproject.io/\\\">https://www.vaultproject.io/</a>), а в основном конфиге указывается путь, по которому их можно достать.</p>\\n<h2>Мониторинг</h2>\\n<p>Zabbix / Cacti / Nagios</p>\\n<p>Проблема в том, что при использовании микросервисов отказ в одном сервисе влечет за собой отказы во всей цепочке сервисов, зависимых от него. В результате из-за одной ошибки получаем десятки алертов.</p>\\n<h3>Как решать?</h3>\\n<ul>\\n<li>вместо полноценного мониторинга - строим dashboard.</li>\\n<li>алерты по статистическим и пороговым значениям вместо бинарных</li>\\n<li>на одном дэшболде сводить графики хост-машин и статистики по docker-демону</li>\\n</ul>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/3ea1f/monitoring-infrastructure.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block;  max-width: 650px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 32.9277566539924%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABwklEQVQozz2R228SURjE9x/WaIzWB+Kl2pj0ReODJja1VWxSbSFYaaUp0ViUtCXlLmErSOmC0L1wWXZZLsv+PKyNJzkzyXyZk/nmSCklQqa5S6q+w7fCNj/kMKeNT6QbUVIXEbqWCh40DYvvlR7JXwYnNRNnNGY6haGRw26GGSgfmRgJpK2zRSLVZTYLS6wl7/Pm+AEh+QmRyjIf5EXONZlBp0e68oeVQ4WVuMzGcRut0+VS7aNVttGyd2ik7uLUXyH1SiXMcpnBmUxXLmMKHgjuC20+mzkO9nDI/LieHxZPgOu62Lbt63iugJnQZ0hK4Db1hwEyC7dI3LhG8uZ1yoEFmo/vUROali9gTybCNOPK7eNU7Gua5n/Fu5pKnXSOTraIMTfO04lkeiZPJ1dEP80yHlj/TMIxmnqMRMyJuPOElmX7T3mu2GDmCB4jBZ+Fefs0RDKWQXSMoqiEXsbYfLHD++dRlGqL2dQhX22zfvibjUSFrZMmuuhV1fr0LvbQfz6ilVti1HiHFF2NEX39mezXvN+D3tI5CH5hPxgntnaA1tT9hOeXJrt5lf2iSrxkMBS/7IoWbPUI63wVs7bOqL3HXz7Q8FplvBTvAAAAAElFTkSuQmCC'); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Как работает мониторинг у них\\\"\\n        title=\\\"\\\"\\n        src=\\\"/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/10273/monitoring-infrastructure.png\\\"\\n        srcset=\\\"/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/9b14a/monitoring-infrastructure.png 163w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/94962/monitoring-infrastructure.png 325w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/10273/monitoring-infrastructure.png 650w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/2fc6f/monitoring-infrastructure.png 975w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/a8a2c/monitoring-infrastructure.png 1300w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/3ea1f/monitoring-infrastructure.png 1315w\\\"\\n        sizes=\\\"(max-width: 650px) 100vw, 650px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p><strong>Diamond</strong> собирает метрики с хост-машины, <strong>cAdvisor</strong> - с контейнера. Потом это все собирается в <strong>InfluxDB</strong> и выводится в <strong>Grafana</strong>.</p>\\n<p>Логи из контейнеров собираются через <strong>Syslog</strong>, аггрегируются в <strong>Logstash</strong>, загоняются в <strong>ElasitSearch</strong>. Оттуда метрики по логам загоняются в <strong>Grafana</strong>, а общая работа с логами ведется в <strong>Kibana</strong>.</p>\\n<p>Трейсинг запросов: <strong>OpenZipkin</strong></p>\\n<p>Альтернативы:</p>\\n<ul>\\n<li>cAdvisor - нет альтернатив, есть <strong>telegraf</strong>, но он пока сырой.</li>\\n<li>InfluxDB - альтернативой может быть <strong>Prometheus</strong>, но он не умеет горизонтально масштабироваться, а InfluxDB умеет, но за деньги. Еще можно сделать сборку метрик в <strong>ElasticSearch</strong>, но он менее производителен и более требователен к ресурсам.</li>\\n</ul>\\n<p>Еще недостатки Prometheus:</p>\\n<ul>\\n<li>хорош, когда нужно собирать данные с готовых стандартных компонент, которых очень много. Если таких компонент мало и плюс к этому нужны метрики по бизнес-логике, то нужно много писать кода на стороне клиента и разбираться во внутренней логике работы Prometheus</li>\\n<li>нельзя использовать как events time-series db, то есть мы не можем кидать туда события, чтобы он потом сам посчитал метрики. Нужно заранее самому считать метрики на клиенте и кидать их уже в готовом виде.</li>\\n<li>сложно делать интегрированные метрики. например, персентили времени отклика клиентам по всем серверам фронтенда. Это сделать в Prometheus невозможно вообще.</li>\\n</ul>\\n<h3>Общие рекомендации</h3>\\n<ol>\\n<li>Сбор метрик - это важно, но еще важнее постоянный анализ полученных данных</li>\\n<li>Система мониторинга должна быть более надежной и масштабируемой, чем то, что она контролирует</li>\\n<li>Система должна быть оптимизирована для распределенных, недолговечных, облачных, контейнеризованных микросервисов</li>\\n<li>Собирайте метрики часто и очень часто, стремитесь к интервалам &#x3C; 10 сек, иначе разовые всплески можно пропустить</li>\\n</ol>\\n<h2>Тестирование</h2>\\n<p>Не нужно тестировать взаимодействие с другими сервисами, нужно тестировать только контракт каждого сервиса.</p>\\n<p>Сложно тестировать ситуации, когда сервис доступен, но отвечает медленно, эпизодически или некорректно. В этой ситуации поможет паттерн <strong>Circuit Breaker</strong> и утилита, его реализующая - <strong>Hystrix</strong>. </p>\\n<p>Полезным будет сделать тестовый кластер из нескольких Raspberry Pi. Из него можно спокойно выдергивать сеть, питание, ставить туда медленные или сбойные флешки.</p>\\n<h2>Хранение</h2>\\n<h3>Что пробовали</h3>\\n<ul>\\n<li>проброс файловой системы в контейнер. Минусы: на одной машине данные пробросили, а на другой их нет. То есть нарушается изоляция.</li>\\n<li>\\\"Собирающие контейнеры\\\" - на бэкэнд ставится проксирующий nginx, он пробрасывает трафик на фронтенд, а у себя собирает всю проходящую через него статику и за несколько итераций получается контейнер, где собраны все данные. Минусы: на монолите прекрасно работает, на микросервисах - нет.</li>\\n<li>Shared data volume - заранее создается волюм, именуется, аттачится к нескольким контейнерам сервисов и таким образом получаем отдельный контейнер с данными, который можно деплоить вместе с контейнером сервисов.</li>\\n<li>Flocker - сторонний менеджер волюмов данных. В отличие от стандартных волюмов докера, которые привязаны к одному серверсу, волюмы Флокера мобильны и могут быть использованы с любым контейнером. Минус: работает конечно через сеть, а большие объемы данных через сеть гонять неудобно.</li>\\n</ul>\\n<h3>Решение</h3>\\n<p>Тут я не очень понял, поэтому далее цитата:</p>\\n<pre><code>Используем **Ceph**, выделили отдельный storage cluster, подняли распределенную файловую систему, часть машин из докер-кластера помечается определенными метками. Когда запускаются контейнеры, то микросервисы, которые требуют такого типа хранения, ориентируются на эти метки. То есть разливаем именно на это подмножество машин.\\n</code></pre>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/91041/file-storage.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block;  max-width: 650px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 120.32085561497325%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAYAAAD6S912AAAACXBIWXMAAAsSAAALEgHS3X78AAAEw0lEQVQ4y31TCVcTSRDOT/btc3cVcVcRV8UDOUIIrFyG3PcEiCsEknBnEsw9IQGXyGEyOYEAIQnfVncU0efbmVevqr6u75up6m5FvdGCN1MgK31nS+ni/9p1HZlvpwRPWkb5rA7Fab2JscA+XosR9AXD1zYYSUAZTXI/GI63/ReMWV/o+/rhjT3sVy6gOLtsYSr0CS+2/FDG1zBI9nrLh067Bnctk7hH/oHbhPuCjse/Gd6gwzpFNV6qXeec3ogf6kAGR9VLKGr1FiaDWbz8sAJVYoMKNqBOBfGXR0DXOyseLzjxZNGF7nkHxx66zehyW6l2k9cyTl90BcNcsM4Em9CEDqAMRjFIbShDUdw1m6EKx6COSRim9lTUsoraZTaaTKN3PYD7DieGIzQSaldJvHExi8PqBZthA/oPOQiJCuzxEhyJMno0blgjMhyU22MlzGzXuLFYSB5Ds7KDPusyZtM12KJFuIir+5CnGZ59FfxMQkSOF7l/u5yiuNDGYgUMzWxg9P0WkfNwJsuwRHLQB/Z4zDhCsgQt/dR+5fzngmOeKBf6ig251rmoLSrDKVVg3jqEZjUDQapcC+puChrC1DJ9zZkocT+xGKN2i/QHJY7NZWpkpzwXpDKs4SNo13bgSlXpo9QyYXreMgmeXDQwKR7BGC3AQHNjfng+Qh+hOMKwAvQUM2uvFzEtHmDMn4aJZsrqzISNk0a2TDOsN1t00un0Z9gNYDemCJM/gsVtmbACx256L92K9/FDCGIavt1yGydjN6VyTuewQS2nFiPYXoph15/kJnki2PEn2vmyhB1fgvIkMr44xzKUs5rd5Xb8cUWCtBBBrXQCxeXZJVYtKxjvHb+2kZ4R7qf6pjD2agyaAQ1UT1VQP1NjgvCxl29gUhv5OltjtYs6D6q5KrVMF3rDuoZlsw9evQce7Tz+eeuG1+Dh+ZJ+AT7jEjy6ecIWKSdMtwC/ycvztvdg1ehH5XMFihbNMLf7GYV/ZaSCEsU5FPcKkCmXP+ZRyhaRTdDFlz61ccJYbZ48yyUxiTxxGK9+TlcPN568nMcVvT8+tfMa6pd1/Oz5kfOdYHAriBa97Lm6urouPMrRkdjPXtddtb4JiMRpXjW/CTLiQeoACW8chgE9Qu9C+BTPotVqoVatYVfcxbz2PWYnZpBe30YlV+HEw+0DJGmHDf3EcQeRjWXRbDShuKhdwNRlhLZTA2u3Gfo/tNB1anEsHyNBhMnbEzDTuuWRicerplVOtDw2Y/reF86fOmg7piHvySR4egFXrwB3/yzm+mbwbmAOzhd2Lphak+DosXGMmfDcjoBzE+zszvbPwE31nDM4BwetFbKF9rGxPrdg6Hcl/u4cwXCHCsbHepwUTniLo3dGoCZspFPNY9EZQPOyCccr+xfOKOfoH+lQ3C+2BW0kOPBrP4buKvH8Vg/03TqcFNuCI3fUUN4ZRO/tVxj4pR9Bl8gFnSTIOKqOIfTcegZt1zRK+6X2DFlbjqc2OJ/ZIPTYYaX5HMtVSCtJmB4aCHNwMz80YsO2ToINCC8dcDwhHuc4YOk2Qc7K7V3ei+4hNBdEQNhEcFbEjpjhu3xaPkFsKQrRFSDbRHg+jNJBke8y29WbnAxxGvSh/wDSyU4rR9AWeAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"File storage\\\"\\n        title=\\\"\\\"\\n        src=\\\"/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/10273/file-storage.png\\\"\\n        srcset=\\\"/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/9b14a/file-storage.png 163w,\\n/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/94962/file-storage.png 325w,\\n/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/10273/file-storage.png 650w,\\n/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/91041/file-storage.png 748w\\\"\\n        sizes=\\\"(max-width: 650px) 100vw, 650px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<h4>Для баз данных:</h4>\\n<ul>\\n<li>выделенные сервера</li>\\n<li>особая конфигурация машин</li>\\n<li>расширенная сеть</li>\\n</ul>\\n<p>Базы крутятся не в докере, потому что у них особые требования к ресурсам.</p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/43a31/database-storage.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block;  max-width: 606px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 103.46534653465346%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAAsSAAALEgHS3X78AAAEa0lEQVQ4y32U+VPaVxTF+YP7Qztdp5mmSdq0Zm2mbdrEJnEXt9EkbqAsEgHFDQRZlXwFMSj78mX5IvLp/cI0k2XSO3PmcbmPx3nvnHsNSlwhEowS2DvgwBckGooSPoj0cm8vjwQl9/bqQX8I5bX85jDOsvOAZbePVY+PhVf7OLbDGM5Ozll4amL8zjTjt6fxOvzYph2M3ppk7NYUu3Yf6y/djPZNdvdYpxzUayrHpwXmPPssekeZcf7FvHeJld0EhmKmzNS9OYx9MwxeH8f1YpOlpysM/2Rk6LoR13MPq6N2ySe69YUnZvTIFJusBqK8OpyRdRBbzIo7ksVQzav03xjgzle/8fNnfTjmnJiHrNz64j4PvnnI9rKXxUEztyXv+/wus4/mUeJHxJMZzPtJ7LGX2CIjmINuNqJFDJV8lecPF98y2rPuszbpYuymPMEvM/hsARwzLgavjTN0w4hlzCH8OsKwhSUUZCNhxOp/gvN4WRhmMNSrDcq5CtlUjtxpnnarjXKo4N8+wOfxk8vk0OoamWSW/JsCpWwZ2m2y5SZL3jN5RxezGys8F0HcOsNauc5l+5J3I51Oc545R1EUctnce7WG2uCi2aLaaLEZy7HkjjNrCWDfTxNKlTG0pKiWatTKPaiCTqvDcTxBMVviUrukXml00ZDb6AT0UJsXeGIFtpVTfKmUsDsjdFLBwCcim8tSLBa5uGihadpbNAXtC41URsV8EMUenWBh+zG2w1XWw6Jyp9PhQ+jRarW6B36IQqFIpVxCSeVZCcRxvp4T64xhja6JyvmPD9Tjv/X/oqheiv8SWCNzmPYHWA07e7b5FMO2KFmtVj9CpVKVTqmSTFfENmE8yUnWQgNiG5O8Y7b3hu8yu7zsKZ7LiY1yeer1OqqqUlNr3VVHs1GXAwss7J4xseZmzLrCtDuCS2eoq6wJmuI1TaygNTQ6cqZypFDMl0A+69680C66a0vU1aOmtdmJF7DtnsiQiOMKZYi+EZXL+Qrul1vSAWtYxx0kwym8Nj9W6Qgdx8EkAWcI87Ct2yUBZ5iO+LZc07oi7CQS+NMKrtgpwaQcWDgvdtvM+OsMQ9eMbMxvM//IxMiNSQavjrMpuWnAIvkUw9cnpM/tveFQamEOhESUEWZdf4htTGIbaT29lfRRdffrBzy80o/X4udF/yK3v7zP79/93c1NwxYZDPd48O2f2CactLQGaRkqK36FV/EXYh2jqLzeU7mYKTFyc5KnPw7z5Idhtpb2WPxnhYFrozy7OsqOycuyMHz8/TP6rwyyPGgheay89aE9NovJNyA+dIjK4kO91RLhEyKeQ2I7R6hFlfBelE3ztmCLzJusDI484c0Yh7tx0sq5bgfy1TYWmfQbiRnW5Nrrry092+j9+2EkT5KkTuVPYhEKxcJ7Nd0Jba1FTnp7Ye8co32L0VW72OaoZ5tmrYk+E9WSSlXY6Qwb1WYXTUG9Uu9+363JnrLs1adTTexzkCzhCaZwyM12D7Mo0t//ArZUxq7Krj6XAAAAAElFTkSuQmCC'); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Database storage\\\"\\n        title=\\\"\\\"\\n        src=\\\"/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/43a31/database-storage.png\\\"\\n        srcset=\\\"/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/e37cb/database-storage.png 163w,\\n/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/869dd/database-storage.png 325w,\\n/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/43a31/database-storage.png 606w\\\"\\n        sizes=\\\"(max-width: 606px) 100vw, 606px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p>Для инстансов БД используется особая схема именования, поэтому каждый сервис знает, как найти свою базу в этом кластере.</p>\\n<p>При этом здесь не нарушается паттерн \\\"одна БД на сервис\\\", потому что внутри сервера есть, например, 10 инстансов монги и каждый микросервис использует свой инстанс.</p>\\n<h2>Обнаружение сервисов</h2>\\n<p>Используется реестр сервисов:</p>\\n<ul>\\n<li><strong>consul</strong> - K/V хранилище, DNS, health-check, работа с несколькими ДЦ из коробки</li>\\n<li>\\n<p><strong>etcd</strong> - K/V хранилище</p>\\n<ul>\\n<li>skydock - Health-Check, регистрация docker-контейнеров</li>\\n<li>skydns - DNS</li>\\n</ul>\\n</li>\\n<li><strong>Apache Zookeeper</strong> - вариация K/V хранилища, сервис блокировок, вариация на тему DNS</li>\\n</ul>\\n<h2>Оркестрация</h2>\\n<ul>\\n<li><strong>docker swarm [1.2.2]</strong> - доставка, регистрация, масштабирование (consul)</li>\\n<li><strong>nomad [0.3.2]</strong> - доставка, регистрация, масштабирование (consul). Самый простой для разработчика, запуск одной командой. Но и сильно ограничен по функционалу.</li>\\n<li>**kubernetes [1.2.4] - доставка, регистрация, масштабирование, LB (etcd)</li>\\n<li><strong>mesos [0.28.1]</strong> (marathon [1.1.1]) - доставка, регистрация, масштабирование, LB (zookeeper, HAProxy)</li>\\n</ul>\\n<h2>Разрешение адресов в момент деплоя</h2>\\n<p>Есть проблема в том, что на момент разработки неизвестно, на какие адреса будет развернута система и следовательно неизвестно, кому по каким адресам обращаться. Эта проблема решена комбинацией nginx и consul.</p>\\n<p>Для consul используется следующий шаблон конфига nginx:</p>\\n<pre><code>{{range services}}\\n    {{ if in .Tags \\\"demo\\\" }}\\n        {{if .Tags | join \\\",\\\" | regexMatch \\\"urlprefix-\\\"}} #urlprefix-/playlist\\n            upstream {{.Name}} { \\n                least_conn; \\n                {{range service .Name}}server {{.Address}}:{{.Port}} max_fails=3 fail_timeout=30 weight=1;{{end}} \\n            } \\n        {{end}}\\n    {{end}}\\n{{end}} \\n\\nserver { \\n    listen 80 default_server; \\n\\n    location /health { \\n        add_header Content-Type text/plain; \\n        return 200 'OK'; \\n    } \\n\\n    {{range services}}\\n        {{ if in .Tags \\\"demo\\\" }}\\n            {{if .Tags | join \\\",\\\" | regexMatch \\\"urlprefix-\\\"}}\\n                {{range .Tags}}{{ if . | contains \\\"urlprefix-\\\" }}\\n                    location {{. | replaceAll \\\"urlprefix-\\\" \\\"\\\"}} { \\n                        {{end}}{{end}} \\n                        proxy_pass http://{{.Name}}/; # / в конце означает обрезку проксируемого пути\\n                        proxy_set_header Host $host; \\n                        proxy_set_header X-Real-IP $remote_addr; \\n                    } \\n            {{end}}\\n        {{end}}\\n    {{end}} \\n}\\n</code></pre>\\n<p>В первом куске шаблона: <code>services</code> - это список <strong>живых</strong> сервисов, полученный из consul. Затем выбирается подмножество с тэгом demo, заем из них выбираем те сервисы, которым нужен роутинг (т.е. те, у которых в тэге есть флаг urlprefix). Если все эти условия выполнились, то генерится списко апстримов.</p>\\n<p>Во втором куске, который в разделе server: эти апстримы подключаются на те роуты, которые заявлены как требующие роутинга.</p>\\n<p>Результат работы шаблона:</p>\\n<pre><code>upstream playlistapi { \\n    least_conn; \\n    server 10.1.1.2:5100 max_fails=3 fail_timeout=30 weight=1; \\n} \\u2028 \\n\\n... \\u2028\\n\\nlocation /playlist { \\n    proxy_pass http://playlistapi/; \\n    proxy_set_header Host $host; \\n    proxy_set_header X-Real-IP $remote_addr; \\n}\\n</code></pre>\\n<p>Исходники примера здесь: <a href=\\\"http://bit.ly/1TE1KdM\\\">http://bit.ly/1TE1KdM</a> </p>\\n<h2>Чек-лист</h2>\\n<ul>\\n<li>Как конфигурировать?</li>\\n<li>Как мониторить и считать метрики?</li>\\n<li>Как сервисы будут находить друг друга</li>\\n<li>Как сервис будет взаимодействовать с остальными частями проекта?</li>\\n<li>Как тестировать сервис и систему целиком?</li>\\n<li>Как обрабатывать сбои связанных сервисов?</li>\\n<li>Как переезжать на новую версию?</li>\\n<li>Как масштабировать под нагрузкой?</li>\\n</ul>\",\"frontmatter\":{\"path\":\"/blog/video/microservices-what-we-learned/\",\"title\":\"Чему мы научились, разрабатывая микросервисы\"}}},\"pathContext\":{}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---blog-video-microservices-what-we-learned-f5a90a189b44da7a91be.js","module.exports = {\"data\":{\"markdownRemark\":{\"html\":\"<p>Кусок архитектуры М-Тех, который занимается нарезкой видео из лайв-потока:</p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/6f71d/microservices-example.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block;  max-width: 650px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 76.51296829971182%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsSAAALEgHS3X78AAACzklEQVQ4y11U207bQBDN//exP0AfqlZqKVQU8VJouYQASSMgaUwuduxgO/gSx/au73ZOxxtCoGOt1lrPnD0zc8YNkFVVhSiKkKYpQubDXc7BOUMcx2IxxpDnOd7YavtalgVY4om9sT4oEQQ+8rTEyOjgdPQRQexsY1crAZxnJbIkR5alyMsccRIjiVJoloTjhx3YgbYGrBnatk2sIni+A92R6Z0LxkmSIo5iyoDT+RBuaMCzlpgONAJL1pelnMBmyIpkCzg3TXAWQ9I6uBjsw3RUnPW/4e+sRQ7AMljg12AXkn2Daf8RRx9OwJYc/1tjk5Ku6wKwvmn01EaSM/SUS+juCCsCLMoKgd5FNO9TvQIM5DayPBEgPPGhOn0kGUejBquqNWDEtzeuSqD5/QaT2ylK8vEcC/bJO/h3B5AdCT/7OwgiR7CfLxWcjz/DZfqaYW2GbiAMQ1TlSjDKkgxSewRdniOqu02XzYxbWAtN+CdZJJpZEvMoDaHazwynFkN34mKqyDCNOXRbRl+9hOtZouO6M0YaZwQa4lT6it7j2Ru51Jk8+YpQhlMzvBq6+HKhwl96CH2OybyL5mgXi9BEc7hH9bzBpnGM+5SFT6VZ61MwLGhVJXgcCJ/Ga63VMnkt3PtmHwalXBATc2aiddTB+UGL9jZ+751DHWviW57lYoku10C11ZPwGrAgEbd+dDDqKgI99ENokoHJvQq5p0LpafAXwXZqnifnDSALavFO8GBckRQCjMw2TG8sOlnQZIxJTk+BgiCxIZktpEUkYn1uo6edgcXeFrDOP0sLDPUOruTv4JmHP4/HkN17VAWVI49xPTyEYt2RTGSc9D4hjBci1vI1NCdUd2Zuhf2Sal6ChZzkU8K+OwSfXoNkKjqqLfrUydmLb63f17EvKW8O670sqMjUuZgt4Tbfg8lrmeQ0p82HfUhUjs0PpV6bOLHo+Qen728FTQDSQgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Microservices example\\\"\\n        title=\\\"\\\"\\n        src=\\\"/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/10273/microservices-example.png\\\"\\n        srcset=\\\"/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/9b14a/microservices-example.png 163w,\\n/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/94962/microservices-example.png 325w,\\n/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/10273/microservices-example.png 650w,\\n/hows-that-again/static/3be3e1dfbb33e6b41c78302ed044597b/6f71d/microservices-example.png 694w\\\"\\n        sizes=\\\"(max-width: 650px) 100vw, 650px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p>Зеленым обозначены микросервисы. Здесь их 17, и это 1/20 часть всей системы. То есть всего около 340 микросервисов. С учетом того, что некоторые сервисы разворачиваются в нескольких экземплярах, получается 1120 нод.</p>\\n<h2>Насколько мелким должен быть микросервис?</h2>\\n<p>Настолько, чтобы он был независимым!</p>\\n<h3>Как проверить независимость:</h3>\\n<ol>\\n<li>Бизнес-задача сервиса должна описываться одним простым предложением. Если не получается - значит можно еще поделить. Например, \\\"показать картинки на таймлайне видеоредактора\\\". Если на эта картинку, например, нужно вотермарк наложить, то это надо делать в этом же сервисе. То есть делить нужно <strong>не технически</strong>, а по бизнес-задачам, иначе грануляция будет слишком высокая. Общие вещи - копипастятся и кладутся в отдельную папку vendor.</li>\\n<li>У сервиса должно быть больше одного потребителя. Если один - значит слишком мелко поделили.</li>\\n<li>Деплой сервиса не должен приводить к деплою других сервисов. Если приводит - значит есть зависимость нашего сервиса от другого, значит слишком мелко поделили.</li>\\n</ol>\\n<h3>Требования к микросервису:</h3>\\n<ul>\\n<li>скрывает внутренние детали реализации</li>\\n<li>деплоится независимо</li>\\n<li>падая, не роняет все остальное</li>\\n<li>легко мониторится</li>\\n<li>реализует бизнес-модель</li>\\n<li>полностью децентрализован</li>\\n</ul>\\n<h2>Конфигурация</h2>\\n<h3>Что пробовали?</h3>\\n<ul>\\n<li>файлы конфигурации при деплое разливать по всем целевым машинам. Минус: сложно менять конфигурацию для одной ноды, количество загрузок конфигов растет линейно с ростом машин.</li>\\n<li>файлы конфигурации класть в контейнер. Минус: при изменении конфига нужно пересобрать и передеплоить контейнет.</li>\\n<li>\\n<p>выставлять переменные окружения (рекомендуется в <a href=\\\"https://12factor.net/\\\">https://12factor.net/</a>)</p>\\n<ul>\\n<li>при сборке контейнера (<code>ENV DB=...</code>). Минус: на этапе деплоя нет информации, где это будет лежать</li>\\n<li>при запуске контейнера. Минус: на этапе запуска нет возможности абстрагироваться от системы запуска.</li>\\n</ul>\\n</li>\\n</ul>\\n<h3>Решение</h3>\\n<ul>\\n<li>общее K/V хранилище (у них consul)</li>\\n<li>сервис должен быть заточен под смену конфигурации</li>\\n<li>\\n<p>микс вариаций с приоритетами:</p>\\n<ul>\\n<li>&#x3C;СЕРВИС>/ (/conf/ms/recorder)</li>\\n<li>&#x3C;СЕРВИС>/&#x3C;ВЕРСИЯ>/ (/conf/ms/recorder/0.12)</li>\\n<li>&#x3C;СЕРВИС>/&#x3C;СРЕДА>/</li>\\n<li>&#x3C;СЕРВИС>/&#x3C;ВЕРСИЯ>/&#x3C;СРЕДА>/</li>\\n<li>&#x3C;СЕРВИС>/&#x3C;ВЕРСИЯ>/&#x3C;НОДА>/</li>\\n</ul>\\n</li>\\n</ul>\\n<p>Конфиги хранятся в тех же репозиториях, что и проекты.</p>\\n<p>Утилита <strong>git2consul</strong> при деплое загружает конфиги в consul.</p>\\n<p>Стартовый путь:\\n<code>/conf/ms/&#x3C;ИМЯ СЕРВИСА>/&#x3C;BUILD NO>/</code></p>\\n<p>Все секретные данные (сертификаты, пароли к внешним сервисам, соль) хранятся в отдельных проектах, загружаются в <strong>Vault</strong> (<a href=\\\"https://www.vaultproject.io/\\\">https://www.vaultproject.io/</a>), а в основном конфиге указывается путь, по которому их можно достать.</p>\\n<h2>Мониторинг</h2>\\n<p>Zabbix / Cacti / Nagios</p>\\n<p>Проблема в том, что при использовании микросервисов отказ в одном сервисе влечет за собой отказы во всей цепочке сервисов, зависимых от него. В результате из-за одной ошибки получаем десятки алертов.</p>\\n<h3>Как решать?</h3>\\n<ul>\\n<li>вместо полноценного мониторинга - строим dashboard.</li>\\n<li>алерты по статистическим и пороговым значениям вместо бинарных</li>\\n<li>на одном дэшболде сводить графики хост-машин и статистики по docker-демону</li>\\n</ul>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/3ea1f/monitoring-infrastructure.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block;  max-width: 650px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 32.9277566539924%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABwklEQVQozz2R228SURjE9x/WaIzWB+Kl2pj0ReODJja1VWxSbSFYaaUp0ViUtCXlLmErSOmC0L1wWXZZLsv+PKyNJzkzyXyZk/nmSCklQqa5S6q+w7fCNj/kMKeNT6QbUVIXEbqWCh40DYvvlR7JXwYnNRNnNGY6haGRw26GGSgfmRgJpK2zRSLVZTYLS6wl7/Pm+AEh+QmRyjIf5EXONZlBp0e68oeVQ4WVuMzGcRut0+VS7aNVttGyd2ik7uLUXyH1SiXMcpnBmUxXLmMKHgjuC20+mzkO9nDI/LieHxZPgOu62Lbt63iugJnQZ0hK4Db1hwEyC7dI3LhG8uZ1yoEFmo/vUROali9gTybCNOPK7eNU7Gua5n/Fu5pKnXSOTraIMTfO04lkeiZPJ1dEP80yHlj/TMIxmnqMRMyJuPOElmX7T3mu2GDmCB4jBZ+Fefs0RDKWQXSMoqiEXsbYfLHD++dRlGqL2dQhX22zfvibjUSFrZMmuuhV1fr0LvbQfz6ilVti1HiHFF2NEX39mezXvN+D3tI5CH5hPxgntnaA1tT9hOeXJrt5lf2iSrxkMBS/7IoWbPUI63wVs7bOqL3HXz7Q8FplvBTvAAAAAElFTkSuQmCC'); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Как работает мониторинг у них\\\"\\n        title=\\\"\\\"\\n        src=\\\"/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/10273/monitoring-infrastructure.png\\\"\\n        srcset=\\\"/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/9b14a/monitoring-infrastructure.png 163w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/94962/monitoring-infrastructure.png 325w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/10273/monitoring-infrastructure.png 650w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/2fc6f/monitoring-infrastructure.png 975w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/a8a2c/monitoring-infrastructure.png 1300w,\\n/hows-that-again/static/acd193c1c80adcdbb8620ef622bea387/3ea1f/monitoring-infrastructure.png 1315w\\\"\\n        sizes=\\\"(max-width: 650px) 100vw, 650px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p><strong>Diamond</strong> собирает метрики с хост-машины, <strong>cAdvisor</strong> - с контейнера. Потом это все собирается в <strong>InfluxDB</strong> и выводится в <strong>Grafana</strong>.</p>\\n<p>Логи из контейнеров собираются через <strong>Syslog</strong>, аггрегируются в <strong>Logstash</strong>, загоняются в <strong>ElasitSearch</strong>. Оттуда метрики по логам загоняются в <strong>Grafana</strong>, а общая работа с логами ведется в <strong>Kibana</strong>.</p>\\n<p>Трейсинг запросов: <strong>OpenZipkin</strong></p>\\n<p>Альтернативы:</p>\\n<ul>\\n<li>cAdvisor - нет альтернатив, есть <strong>telegraf</strong>, но он пока сырой.</li>\\n<li>InfluxDB - альтернативой может быть <strong>Prometheus</strong>, но он не умеет горизонтально масштабироваться, а InfluxDB умеет, но за деньги. Еще можно сделать сборку метрик в <strong>ElasticSearch</strong>, но он менее производителен и более требователен к ресурсам.</li>\\n</ul>\\n<p>Еще недостатки Prometheus:</p>\\n<ul>\\n<li>хорош, когда нужно собирать данные с готовых стандартных компонент, которых очень много. Если таких компонент мало и плюс к этому нужны метрики по бизнес-логике, то нужно много писать кода на стороне клиента и разбираться во внутренней логике работы Prometheus</li>\\n<li>нельзя использовать как events time-series db, то есть мы не можем кидать туда события, чтобы он потом сам посчитал метрики. Нужно заранее самому считать метрики на клиенте и кидать их уже в готовом виде.</li>\\n<li>сложно делать интегрированные метрики. например, персентили времени отклика клиентам по всем серверам фронтенда. Это сделать в Prometheus невозможно вообще.</li>\\n</ul>\\n<h3>Общие рекомендации</h3>\\n<ol>\\n<li>Сбор метрик - это важно, но еще важнее постоянный анализ полученных данных</li>\\n<li>Система мониторинга должна быть более надежной и масштабируемой, чем то, что она контролирует</li>\\n<li>Система должна быть оптимизирована для распределенных, недолговечных, облачных, контейнеризованных микросервисов</li>\\n<li>Собирайте метрики часто и очень часто, стремитесь к интервалам &#x3C; 10 сек, иначе разовые всплески можно пропустить</li>\\n</ol>\\n<h2>Тестирование</h2>\\n<p>Не нужно тестировать взаимодействие с другими сервисами, нужно тестировать только контракт каждого сервиса.</p>\\n<p>Сложно тестировать ситуации, когда сервис доступен, но отвечает медленно, эпизодически или некорректно. В этой ситуации поможет паттерн <strong>Circuit Breaker</strong> и утилита, его реализующая - <strong>Hystrix</strong>. </p>\\n<p>Полезным будет сделать тестовый кластер из нескольких Raspberry Pi. Из него можно спокойно выдергивать сеть, питание, ставить туда медленные или сбойные флешки.</p>\\n<h2>Хранение</h2>\\n<h3>Что пробовали</h3>\\n<ul>\\n<li>проброс файловой системы в контейнер. Минусы: на одной машине данные пробросили, а на другой их нет. То есть нарушается изоляция.</li>\\n<li>\\\"Собирающие контейнеры\\\" - на бэкэнд ставится проксирующий nginx, он пробрасывает трафик на фронтенд, а у себя собирает всю проходящую через него статику и за несколько итераций получается контейнер, где собраны все данные. Минусы: на монолите прекрасно работает, на микросервисах - нет.</li>\\n<li>Shared data volume - заранее создается волюм, именуется, аттачится к нескольким контейнерам сервисов и таким образом получаем отдельный контейнер с данными, который можно деплоить вместе с контейнером сервисов.</li>\\n<li>Flocker - сторонний менеджер волюмов данных. В отличие от стандартных волюмов докера, которые привязаны к одному серверсу, волюмы Флокера мобильны и могут быть использованы с любым контейнером. Минус: работает конечно через сеть, а большие объемы данных через сеть гонять неудобно.</li>\\n</ul>\\n<h3>Решение</h3>\\n<p>Тут я не очень понял, поэтому далее цитата:</p>\\n<pre><code>Используем **Ceph**, выделили отдельный storage cluster, подняли распределенную файловую систему, часть машин из докер-кластера помечается определенными метками. Когда запускаются контейнеры, то микросервисы, которые требуют такого типа хранения, ориентируются на эти метки. То есть разливаем именно на это подмножество машин.\\n</code></pre>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/91041/file-storage.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block;  max-width: 650px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 120.32085561497325%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAYAAAD6S912AAAACXBIWXMAAAsSAAALEgHS3X78AAAEw0lEQVQ4y31TCVcTSRDOT/btc3cVcVcRV8UDOUIIrFyG3PcEiCsEknBnEsw9IQGXyGEyOYEAIQnfVncU0efbmVevqr6u75up6m5FvdGCN1MgK31nS+ni/9p1HZlvpwRPWkb5rA7Fab2JscA+XosR9AXD1zYYSUAZTXI/GI63/ReMWV/o+/rhjT3sVy6gOLtsYSr0CS+2/FDG1zBI9nrLh067Bnctk7hH/oHbhPuCjse/Gd6gwzpFNV6qXeec3ogf6kAGR9VLKGr1FiaDWbz8sAJVYoMKNqBOBfGXR0DXOyseLzjxZNGF7nkHxx66zehyW6l2k9cyTl90BcNcsM4Em9CEDqAMRjFIbShDUdw1m6EKx6COSRim9lTUsoraZTaaTKN3PYD7DieGIzQSaldJvHExi8PqBZthA/oPOQiJCuzxEhyJMno0blgjMhyU22MlzGzXuLFYSB5Ds7KDPusyZtM12KJFuIir+5CnGZ59FfxMQkSOF7l/u5yiuNDGYgUMzWxg9P0WkfNwJsuwRHLQB/Z4zDhCsgQt/dR+5fzngmOeKBf6ig251rmoLSrDKVVg3jqEZjUDQapcC+puChrC1DJ9zZkocT+xGKN2i/QHJY7NZWpkpzwXpDKs4SNo13bgSlXpo9QyYXreMgmeXDQwKR7BGC3AQHNjfng+Qh+hOMKwAvQUM2uvFzEtHmDMn4aJZsrqzISNk0a2TDOsN1t00un0Z9gNYDemCJM/gsVtmbACx256L92K9/FDCGIavt1yGydjN6VyTuewQS2nFiPYXoph15/kJnki2PEn2vmyhB1fgvIkMr44xzKUs5rd5Xb8cUWCtBBBrXQCxeXZJVYtKxjvHb+2kZ4R7qf6pjD2agyaAQ1UT1VQP1NjgvCxl29gUhv5OltjtYs6D6q5KrVMF3rDuoZlsw9evQce7Tz+eeuG1+Dh+ZJ+AT7jEjy6ecIWKSdMtwC/ycvztvdg1ehH5XMFihbNMLf7GYV/ZaSCEsU5FPcKkCmXP+ZRyhaRTdDFlz61ccJYbZ48yyUxiTxxGK9+TlcPN568nMcVvT8+tfMa6pd1/Oz5kfOdYHAriBa97Lm6urouPMrRkdjPXtddtb4JiMRpXjW/CTLiQeoACW8chgE9Qu9C+BTPotVqoVatYVfcxbz2PWYnZpBe30YlV+HEw+0DJGmHDf3EcQeRjWXRbDShuKhdwNRlhLZTA2u3Gfo/tNB1anEsHyNBhMnbEzDTuuWRicerplVOtDw2Y/reF86fOmg7piHvySR4egFXrwB3/yzm+mbwbmAOzhd2Lphak+DosXGMmfDcjoBzE+zszvbPwE31nDM4BwetFbKF9rGxPrdg6Hcl/u4cwXCHCsbHepwUTniLo3dGoCZspFPNY9EZQPOyCccr+xfOKOfoH+lQ3C+2BW0kOPBrP4buKvH8Vg/03TqcFNuCI3fUUN4ZRO/tVxj4pR9Bl8gFnSTIOKqOIfTcegZt1zRK+6X2DFlbjqc2OJ/ZIPTYYaX5HMtVSCtJmB4aCHNwMz80YsO2ToINCC8dcDwhHuc4YOk2Qc7K7V3ei+4hNBdEQNhEcFbEjpjhu3xaPkFsKQrRFSDbRHg+jNJBke8y29WbnAxxGvSh/wDSyU4rR9AWeAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"File storage\\\"\\n        title=\\\"\\\"\\n        src=\\\"/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/10273/file-storage.png\\\"\\n        srcset=\\\"/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/9b14a/file-storage.png 163w,\\n/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/94962/file-storage.png 325w,\\n/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/10273/file-storage.png 650w,\\n/hows-that-again/static/634eeff9e8ae762154020f44bcb3ff98/91041/file-storage.png 748w\\\"\\n        sizes=\\\"(max-width: 650px) 100vw, 650px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<h4>Для баз данных:</h4>\\n<ul>\\n<li>выделенные сервера</li>\\n<li>особая конфигурация машин</li>\\n<li>расширенная сеть</li>\\n</ul>\\n<p>Базы крутятся не в докере, потому что у них особые требования к ресурсам.</p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/43a31/database-storage.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block;  max-width: 606px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 103.46534653465346%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAAsSAAALEgHS3X78AAAEa0lEQVQ4y32U+VPaVxTF+YP7Qztdp5mmSdq0Zm2mbdrEJnEXt9EkbqAsEgHFDQRZlXwFMSj78mX5IvLp/cI0k2XSO3PmcbmPx3nvnHsNSlwhEowS2DvgwBckGooSPoj0cm8vjwQl9/bqQX8I5bX85jDOsvOAZbePVY+PhVf7OLbDGM5Ozll4amL8zjTjt6fxOvzYph2M3ppk7NYUu3Yf6y/djPZNdvdYpxzUayrHpwXmPPssekeZcf7FvHeJld0EhmKmzNS9OYx9MwxeH8f1YpOlpysM/2Rk6LoR13MPq6N2ySe69YUnZvTIFJusBqK8OpyRdRBbzIo7ksVQzav03xjgzle/8fNnfTjmnJiHrNz64j4PvnnI9rKXxUEztyXv+/wus4/mUeJHxJMZzPtJ7LGX2CIjmINuNqJFDJV8lecPF98y2rPuszbpYuymPMEvM/hsARwzLgavjTN0w4hlzCH8OsKwhSUUZCNhxOp/gvN4WRhmMNSrDcq5CtlUjtxpnnarjXKo4N8+wOfxk8vk0OoamWSW/JsCpWwZ2m2y5SZL3jN5RxezGys8F0HcOsNauc5l+5J3I51Oc545R1EUctnce7WG2uCi2aLaaLEZy7HkjjNrCWDfTxNKlTG0pKiWatTKPaiCTqvDcTxBMVviUrukXml00ZDb6AT0UJsXeGIFtpVTfKmUsDsjdFLBwCcim8tSLBa5uGihadpbNAXtC41URsV8EMUenWBh+zG2w1XWw6Jyp9PhQ+jRarW6B36IQqFIpVxCSeVZCcRxvp4T64xhja6JyvmPD9Tjv/X/oqheiv8SWCNzmPYHWA07e7b5FMO2KFmtVj9CpVKVTqmSTFfENmE8yUnWQgNiG5O8Y7b3hu8yu7zsKZ7LiY1yeer1OqqqUlNr3VVHs1GXAwss7J4xseZmzLrCtDuCS2eoq6wJmuI1TaygNTQ6cqZypFDMl0A+69680C66a0vU1aOmtdmJF7DtnsiQiOMKZYi+EZXL+Qrul1vSAWtYxx0kwym8Nj9W6Qgdx8EkAWcI87Ct2yUBZ5iO+LZc07oi7CQS+NMKrtgpwaQcWDgvdtvM+OsMQ9eMbMxvM//IxMiNSQavjrMpuWnAIvkUw9cnpM/tveFQamEOhESUEWZdf4htTGIbaT29lfRRdffrBzy80o/X4udF/yK3v7zP79/93c1NwxYZDPd48O2f2CactLQGaRkqK36FV/EXYh2jqLzeU7mYKTFyc5KnPw7z5Idhtpb2WPxnhYFrozy7OsqOycuyMHz8/TP6rwyyPGgheay89aE9NovJNyA+dIjK4kO91RLhEyKeQ2I7R6hFlfBelE3ztmCLzJusDI484c0Yh7tx0sq5bgfy1TYWmfQbiRnW5Nrrry092+j9+2EkT5KkTuVPYhEKxcJ7Nd0Jba1FTnp7Ye8co32L0VW72OaoZ5tmrYk+E9WSSlXY6Qwb1WYXTUG9Uu9+363JnrLs1adTTexzkCzhCaZwyM12D7Mo0t//ArZUxq7Krj6XAAAAAElFTkSuQmCC'); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Database storage\\\"\\n        title=\\\"\\\"\\n        src=\\\"/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/43a31/database-storage.png\\\"\\n        srcset=\\\"/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/e37cb/database-storage.png 163w,\\n/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/869dd/database-storage.png 325w,\\n/hows-that-again/static/e8c57b72b64c647c8a4354504d91a9d8/43a31/database-storage.png 606w\\\"\\n        sizes=\\\"(max-width: 606px) 100vw, 606px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p>Для инстансов БД используется особая схема именования, поэтому каждый сервис знает, как найти свою базу в этом кластере.</p>\\n<p>При этом здесь не нарушается паттерн \\\"одна БД на сервис\\\", потому что внутри сервера есть, например, 10 инстансов монги и каждый микросервис использует свой инстанс.</p>\\n<h2>Обнаружение сервисов</h2>\\n<p>Используется реестр сервисов:</p>\\n<ul>\\n<li><strong>consul</strong> - K/V хранилище, DNS, health-check, работа с несколькими ДЦ из коробки</li>\\n<li>\\n<p><strong>etcd</strong> - K/V хранилище</p>\\n<ul>\\n<li>skydock - Health-Check, регистрация docker-контейнеров</li>\\n<li>skydns - DNS</li>\\n</ul>\\n</li>\\n<li><strong>Apache Zookeeper</strong> - вариация K/V хранилища, сервис блокировок, вариация на тему DNS</li>\\n</ul>\\n<h2>Оркестрация</h2>\\n<ul>\\n<li><strong>docker swarm [1.2.2]</strong> - доставка, регистрация, масштабирование (consul)</li>\\n<li><strong>nomad [0.3.2]</strong> - доставка, регистрация, масштабирование (consul). Самый простой для разработчика, запуск одной командой. Но и сильно ограничен по функционалу.</li>\\n<li>**kubernetes [1.2.4] - доставка, регистрация, масштабирование, LB (etcd)</li>\\n<li><strong>mesos [0.28.1]</strong> (marathon [1.1.1]) - доставка, регистрация, масштабирование, LB (zookeeper, HAProxy)</li>\\n</ul>\\n<h2>Разрешение адресов в момент деплоя</h2>\\n<p>Есть проблема в том, что на момент разработки неизвестно, на какие адреса будет развернута система и следовательно неизвестно, кому по каким адресам обращаться. Эта проблема решена комбинацией nginx и consul.</p>\\n<p>Для consul используется следующий шаблон конфига nginx:</p>\\n<pre><code>{{range services}}\\n    {{ if in .Tags \\\"demo\\\" }}\\n        {{if .Tags | join \\\",\\\" | regexMatch \\\"urlprefix-\\\"}} #urlprefix-/playlist\\n            upstream {{.Name}} { \\n                least_conn; \\n                {{range service .Name}}server {{.Address}}:{{.Port}} max_fails=3 fail_timeout=30 weight=1;{{end}} \\n            } \\n        {{end}}\\n    {{end}}\\n{{end}} \\n\\nserver { \\n    listen 80 default_server; \\n\\n    location /health { \\n        add_header Content-Type text/plain; \\n        return 200 'OK'; \\n    } \\n\\n    {{range services}}\\n        {{ if in .Tags \\\"demo\\\" }}\\n            {{if .Tags | join \\\",\\\" | regexMatch \\\"urlprefix-\\\"}}\\n                {{range .Tags}}{{ if . | contains \\\"urlprefix-\\\" }}\\n                    location {{. | replaceAll \\\"urlprefix-\\\" \\\"\\\"}} { \\n                        {{end}}{{end}} \\n                        proxy_pass http://{{.Name}}/; # / в конце означает обрезку проксируемого пути\\n                        proxy_set_header Host $host; \\n                        proxy_set_header X-Real-IP $remote_addr; \\n                    } \\n            {{end}}\\n        {{end}}\\n    {{end}} \\n}\\n</code></pre>\\n<p>В первом куске шаблона: <code>services</code> - это список <strong>живых</strong> сервисов, полученный из consul. Затем выбирается подмножество с тэгом demo, заем из них выбираем те сервисы, которым нужен роутинг (т.е. те, у которых в тэге есть флаг urlprefix). Если все эти условия выполнились, то генерится списко апстримов.</p>\\n<p>Во втором куске, который в разделе server: эти апстримы подключаются на те роуты, которые заявлены как требующие роутинга.</p>\\n<p>Результат работы шаблона:</p>\\n<pre><code>upstream playlistapi { \\n    least_conn; \\n    server 10.1.1.2:5100 max_fails=3 fail_timeout=30 weight=1; \\n} \\u2028 \\n\\n... \\u2028\\n\\nlocation /playlist { \\n    proxy_pass http://playlistapi/; \\n    proxy_set_header Host $host; \\n    proxy_set_header X-Real-IP $remote_addr; \\n}\\n</code></pre>\\n<p>Исходники примера здесь: <a href=\\\"http://bit.ly/1TE1KdM\\\">http://bit.ly/1TE1KdM</a> </p>\\n<h2>Чек-лист</h2>\\n<ul>\\n<li>Как конфигурировать?</li>\\n<li>Как мониторить и считать метрики?</li>\\n<li>Как сервисы будут находить друг друга</li>\\n<li>Как сервис будет взаимодействовать с остальными частями проекта?</li>\\n<li>Как тестировать сервис и систему целиком?</li>\\n<li>Как обрабатывать сбои связанных сервисов?</li>\\n<li>Как переезжать на новую версию?</li>\\n<li>Как масштабировать под нагрузкой?</li>\\n</ul>\",\"frontmatter\":{\"path\":\"/blog/video/microservices-what-we-learned/\",\"title\":\"Чему мы научились, разрабатывая микросервисы\"}}},\"pathContext\":{}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/blog-video-microservices-what-we-learned.json\n// module id = 488\n// module chunks = 229226591425695"],"sourceRoot":""}